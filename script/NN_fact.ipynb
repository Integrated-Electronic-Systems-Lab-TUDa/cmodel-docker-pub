{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "\n",
    "\n",
    "# dir_script = (\"/script\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import config\n",
    "\n",
    "import Run_qlattice\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import NN_run\n",
    "import prepare_data as data_source\n",
    "import mongodb_driver\n",
    "import requests\n",
    "import feyn\n",
    "import pickle\n",
    "from tensorflow import keras \n",
    "from keras import layers\n",
    "import utility_func as uf\n",
    "import sklearn.metrics as skm \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy.ma as ma\n",
    "from sklearn.model_selection import train_test_split\n",
    "import NN_build_small\n",
    "import tensorflow as tf\n",
    "import kennard_stone as ks\n",
    "\n",
    "import rfet_model as rmdl\n",
    "\n",
    "import add_gaussian_noise\n",
    "import transfer_func as transf\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum number of models to generate in parallel, with different seeds\n",
    "i_max = 1\n",
    "\n",
    "# PyTaurus run to extract the data from. \"run_regular_grid_common_goal\" is used in the dissertation\n",
    "str_run         = \"run_regular_grid_common_goal\"\n",
    "# the respective gate sweep to be used\n",
    "str_electrode   = \"lat2\"\n",
    "\n",
    "# minimum stepsize, refers to $\\Delta V_{sweep}$ in section 4.2.1\n",
    "min_step = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdb = mongodb_driver.mdb_connect(\"mdb_cmodel\", \"felixl\", \"mdbpass\")\n",
    "mdb = mongodb_driver.mdb_connect(\"mdb_cmodel\", \"root\", \"t(jWbtU4DRnuXqdhK\")\n",
    "\n",
    "mdb_col = mdb[\"NN\"]\n",
    "\n",
    "\n",
    "par_offset = 8\n",
    "par_slope = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for i in range(i_max):\n",
    "    pars = {\n",
    "        \"data_type\"                         : 'DC',\n",
    "        \"data_path\"                         : f\"{config.dir_data}/{str_run}_{str_electrode}_minstep{min_step}_export.pkl\",\n",
    "        \"save_path\"                         : f\"NN_models/mx/lin/\",\n",
    "\n",
    "        \"mdl_type\"                          : 'lin',\n",
    "        \"mdl_arch\"                          : 'NN',\n",
    "\n",
    "        # \"augmentation_type\"                 : \"gauss_noise_3\",\n",
    "        \"augmentation_type\"                 : None,\n",
    "        \"train_test_split_random_seed\"      : i,\n",
    "        \"tf_epochs\"                         : 500,\n",
    "        \"tf_es_patience\"                    : 400,\n",
    "        \"tf_es_delta\"                       : 1E-07,\n",
    "        \"tf_n_neurons\"                      : 35,\n",
    "        \n",
    "        \"user\"                              : os.getlogin(),\n",
    "        \"comment\"                           : \"minimal example test\"\n",
    "    }\n",
    "\n",
    "    Xlabels =   [\"Vlat21\", \"Vfglat1\", \"Vtglat1\"]\n",
    "    Ylabel  =   [\"ids\"]\n",
    "\n",
    "    ### create MongoDB document\n",
    "\n",
    "\n",
    "    Xlabels =   [\"Vlat21\", \"Vfglat1\", \"Vtglat1\"]\n",
    "    Ylabel  =   [\"ids\"]\n",
    "\n",
    "\n",
    "    data_raw = data_source.load_dc_ramp_data(pars['data_path'], Xlabels =[\"Vlat21\", \"Vfglat1\", \"Vtglat1\", \"Vbglat1\"], Ylabel=[\"ids\"],scale_factor=1E3)\n",
    "\n",
    "    # inv_data = data_source.load_from_DC_sim(\"data/diss/eval/INV/inv_n.plt\",Xlabels, Ylabel, scale_factor=1 )\n",
    "\n",
    "    # data_raw = pd.concat([inv_data, data_raw], ignore_index=True)\n",
    "\n",
    "    data_raw_lin = data_raw[data_raw[\"ids\"].abs() > 1e-8 ].copy()\n",
    "    # data_raw_lin = data_raw.copy()\n",
    "    # set current sign = vds sign\n",
    "    # data_raw_lin[\"ids\"] = data_raw_lin.apply(lambda row: abs(row[\"ids\"])*(-1)* np.sign(row[\"Vlat21\"]), axis=1)\n",
    "\n",
    "    lin_prescale= 1\n",
    "\n",
    "    data_raw_lin[\"ids\"] = data_raw_lin[\"ids\"].apply(lambda x: x*lin_prescale)\n",
    "\n",
    "    trainVal = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    train_data = pd.DataFrame()\n",
    "    val_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # --- creating the validation dataset for this NN train run, can be different random seed every time\n",
    "    ks_x_trainVal, ks_x_test, ks_y_trainVal, ks_y_test = ks.train_test_split(data_raw_lin[Xlabels], data_raw_lin[Ylabel],test_size=0.10)\n",
    "\n",
    "    ks_x_train, ks_x_val, ks_y_train, ks_y_val = ks.train_test_split(ks_x_trainVal, ks_y_trainVal,test_size=0.15)\n",
    "\n",
    "    train_data[Xlabels] = ks_x_train\n",
    "    train_data[Ylabel] = ks_y_train\n",
    "\n",
    "    val_data[Xlabels] = ks_x_val\n",
    "    val_data[Ylabel] = ks_y_val\n",
    "\n",
    "    test_data[Xlabels] = ks_x_test\n",
    "    test_data[Ylabel] = ks_y_test\n",
    "\n",
    "    X = train_data.loc[:,Xlabels].to_numpy()\n",
    "    y_lin = train_data.loc[:,Ylabel].to_numpy()\n",
    "\n",
    "    X_val = val_data.loc[:,Xlabels].to_numpy()\n",
    "    y_val_lin = val_data.loc[:,Ylabel].to_numpy()\n",
    "\n",
    "    X_test = test_data.loc[:,Xlabels].to_numpy()\n",
    "    y_test_lin = test_data.loc[:,Ylabel].to_numpy()\n",
    "\n",
    "    # ----building input layer----\n",
    "    x = layers.Input(shape= (3),name =\"InputLayer\")\n",
    "\n",
    "    # ---- hidden layser ----\n",
    "    zOne = layers.Dense(units=pars[\"tf_n_neurons\"], activation = \"tanh\", name = \"HiddenLayer1\")(x)\n",
    "    zTwo = layers.Dense(units=pars[\"tf_n_neurons\"], activation = \"tanh\", name = \"HiddenLayer2\")(zOne)\n",
    "    zThree = layers.Dense(units=pars[\"tf_n_neurons\"], activation = \"tanh\", name = \"HiddenLayer3\")(zTwo)\n",
    "\n",
    "    #output Layer\n",
    "    y = layers.Dense(units = 1, activation = 'linear', name = \"OutputLayer\")(zThree)\n",
    "\n",
    "    #build model\n",
    "    mdl = keras.Model(inputs=x,outputs=y, name = \"NN_model\")\n",
    "    print(mdl.summary())\n",
    "\n",
    "    my_metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "    my_loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "\n",
    "    mdl.compile(optimizer=keras.optimizers.Adam(learning_rate=0.003636),loss=my_loss,metrics = my_metrics)\n",
    "\n",
    "    # compiled_mdl = NN_build_small.build_tanh_model(dim_in = 3, n_neurons = pars[\"tf_n_neurons\"])\n",
    "\n",
    "    callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=pars[\"tf_es_patience\"],min_delta=pars[\"tf_es_delta\"],verbose=1,mode='min',restore_best_weights=True),\n",
    "            tf.keras.callbacks.TensorBoard(\"./NN_tuning_res/RuntimeTest_logs\")]\n",
    "\n",
    "    history = mdl.fit(X,y_lin,\n",
    "                               batch_size = 100,\n",
    "                               epochs = pars[\"tf_epochs\"],\n",
    "                               callbacks = callbacks,\n",
    "                               validation_data=(X_val,y_val_lin),\n",
    "                               verbose = 2,\n",
    "                               shuffle = True)\n",
    "\n",
    "\n",
    "    #----- saving the model ----\n",
    "    # mdl,loss,MAE,R2, sMAPE, X, y = NN_run.mx_run_NN_lin(\n",
    "    #                 save_path = pars['save_path'], \n",
    "    #                 epochs = pars['tf_epochs'],\n",
    "    #                 augmentation_type = pars['augmentation_type'],\n",
    "    #                 random_seed = pars[\"train_test_split_random_seed\"],\n",
    "    #                 data = data,\n",
    "    #                 Xlabels = Xlabels,\n",
    "    #                 Ylabel = Ylabel,\n",
    "    #                 es_patience = pars[\"tf_es_patience\"],\n",
    "    #                 es_delta = pars[\"tf_es_delta\"], \n",
    "    #                 _id = _id_lin,\n",
    "    #                 # splitting_algorithm=\"kennard_stone\",\n",
    "    #                 test_size = 0.0,\n",
    "    #                 n_neurons = pars[\"tf_n_neurons\"])\n",
    "    y_pred = mdl.predict(X_test,batch_size = 50)\n",
    "\n",
    "    loss,MAE= mdl.evaluate(X_test,y_test_lin,batch_size = 100)  # returns loss and metrics\n",
    "\n",
    "    sMAPE = uf.Smape(y_test_lin, y_pred)\n",
    "\n",
    "    pars[\"lin_prescale\"]                     = lin_prescale\n",
    "\n",
    "    _id_lin = mdb_col.insert_one(pars)\n",
    "    _id_lin = str(_id_lin.inserted_id)\n",
    "    print(_id_lin)\n",
    "\n",
    "    mdl.save(pars[\"save_path\"] + \"/\" + str(_id_lin))\n",
    "\n",
    "    res.append({\"_id\" : _id_lin, \"i\": i,\"MAE\": MAE,  \"loss\": loss, \"model\" : mdl, \"sMAPE\" : sMAPE})\n",
    "    print(f\"i: {i}\\nloss {loss}\\nMAE {MAE}\\n\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mdl.predict(test_data[Xlabels])\n",
    "\n",
    "y_pred = np.array(y_pred/lin_prescale)\n",
    "y_true = np.array(test_data[\"ids\"]/lin_prescale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_res_lin = pd.DataFrame(res)\n",
    "\n",
    "pd_res_lin.sort_values(by=[\"loss\"], inplace=True, ascending=True)\n",
    "pd_res_lin.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(pd_res_lin)\n",
    "\n",
    "# get best model\n",
    "best_model_lin_id = pd_res_lin[\"loss\"].idxmin()\n",
    "best_model_lin = pd_res_lin.loc[best_model_lin_id, \"model\"]\n",
    "\n",
    "_id_lin = pd_res_lin.loc[best_model_lin_id, \"_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "# x_vals = test_data['time']\n",
    "\n",
    "fig, (ax, ax2) = plt.subplots(2, sharex=True)\n",
    "#fig,ax = plt.subplots()\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "#ax2 = ax.twinx()\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "\n",
    "ax2.set_xlabel(\"index\")\n",
    "#ax.set_xlabel(\"time\")\n",
    "\n",
    "# ax.grid()\n",
    "ax2.grid()\n",
    "\n",
    "\n",
    "ys = pd.DataFrame()\n",
    "ys[\"true\"] = y_true\n",
    "\n",
    "ys[\"pred\"] =  mdl.predict(test_data[Xlabels])/lin_prescale\n",
    "# y_true_series = pd.Series(y_true)\n",
    "# y_pred_series = pd.Series(y_pred)\n",
    "\n",
    "ys.sort_values(by=[\"true\"],inplace=True)\n",
    "ys.reset_index(inplace=True)\n",
    "\n",
    "ax.scatter(ys.index,ys[\"true\"].abs(), label=\"data\")\n",
    "ax.scatter(ys.index,ys[\"pred\"].abs(), label=\"model\")\n",
    "\n",
    "error = ys[\"true\"] - ys[\"pred\"]\n",
    "\n",
    "ax2.scatter(ys.index,abs(error))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 current model: %.6f' % skm.r2_score(y_true,y_pred))\n",
    "print('MAE current model: %.4E' %(skm.mean_absolute_error(y_true,y_pred)))\n",
    "print('MSE current model: %.4E' %(skm.mean_squared_error(y_true,y_pred)))\n",
    "print('MAPE current model: %.4E' %(skm.mean_absolute_percentage_error(y_true,y_pred)))\n",
    "print('sMAPE current model: %.6f' %(uf.Smape(y_true.reshape([-1,1]),y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(2, sharex=True)\n",
    "\n",
    "ax.scatter(data_raw_lin.index,data_raw_lin[\"ids\"].abs(), label=\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_data = data_source.load_from_DC_sim(\"refs/inv_p.plt\",Xlabels, Ylabel, scale_factor=1 )\n",
    "\n",
    "inv_data.sort_values(by=[\"Vlat21\"], inplace=True,ignore_index=True)\n",
    "inv_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, (ax, ax2,ax3,ax4) = plt.subplots(4, sharex=True)\n",
    "#fig,ax = plt.subplots()\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "#ax2 = ax.twinx()\n",
    "\n",
    "\n",
    "# ax2.set_yscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "#ax.set_xlabel(\"time\")\n",
    "\n",
    "# ax.grid()\n",
    "inv_data[\"pred\"] = mdl.predict(inv_data[Xlabels])  * 1/lin_prescale * 1/1E3\n",
    "\n",
    "# ax.plot(inv_data.index,inv_data[\"pred\"], label=\"Ilin\")\n",
    "ax.plot(inv_data.index,inv_data[\"pred\"], label=\"Ilin\")\n",
    "ax.plot(inv_data.index,-inv_data[\"pred\"], label=\"Ilin-\")\n",
    "\n",
    "ax.plot(inv_data.index,-inv_data[\"ids\"], label=\"ref\")\n",
    "\n",
    "ax2.plot(inv_data.index,inv_data[\"Vlat21\"])\n",
    "ax3.plot(inv_data.index,inv_data[\"Vfglat1\"])\n",
    "\n",
    "# ax.set_ylim([1E-6, 1])\n",
    "# error = abs(val_data[\"ids\"].values.reshape(-1,1) - I)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGARITHMIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for i in range(i_max):\n",
    "    pars = {\n",
    "        \"data_type\"                         : 'DC',\n",
    "        \"data_path\"                         : f\"{config.dir_data}/{str_run}_{str_electrode}_minstep{min_step}_export.pkl\",\n",
    "        \"save_path\"                         : f\"NN_models/mx/log/\",\n",
    "\n",
    "        \"mdl_type\"                          : 'log',\n",
    "        \"mdl_arch\"                          : 'NN',\n",
    "\n",
    "        # \"augmentation_type\"                 : \"gauss_noise_3\",\n",
    "        \"augmentation_type\"                 : None,\n",
    "        \"train_test_split_random_seed\"      : i,\n",
    "        \"tf_epochs\"                         : 500,\n",
    "        \"tf_es_patience\"                    : 400,\n",
    "        \"tf_es_delta\"                       : 1E-07,\n",
    "        \"tf_n_neurons\"                      : 34,\n",
    "        \n",
    "        \"user\"                              : os.getlogin(),\n",
    "        \"comment\"                           : \"minimal example test\"\n",
    "    }\n",
    "\n",
    "    Xlabels =   [\"Vlat21\", \"Vfglat1\", \"Vtglat1\"]\n",
    "    Ylabel  =   [\"ids\"]\n",
    "\n",
    "    ### create MongoDB document\n",
    "\n",
    "\n",
    "    Xlabels =   [\"Vlat21\", \"Vfglat1\", \"Vtglat1\"]\n",
    "    Ylabel  =   [\"ids\"]\n",
    "\n",
    "\n",
    "    data_raw = data_source.load_dc_ramp_data(pars['data_path'], Xlabels =[\"Vlat21\", \"Vfglat1\", \"Vtglat1\", \"Vbglat1\"], Ylabel=[\"ids\"],scale_factor=1E3)\n",
    "\n",
    "    # inv_data = data_source.load_from_DC_sim(\"data/diss/eval/INV/inv_n.plt\",Xlabels, Ylabel, scale_factor=1 )\n",
    "\n",
    "    # data_raw = pd.concat([inv_data, data_raw], ignore_index=True)\n",
    "\n",
    "    # data_raw_lin = data_raw[data_raw[\"ids\"].abs() > 1e-7 ].copy()\n",
    "    # data_raw_lin = data_raw.copy()\n",
    "    # set current sign = vds sign\n",
    "    # data_raw_lin[\"ids\"] = data_raw_lin.apply(lambda row: abs(row[\"ids\"])*(-1)* np.sign(row[\"Vlat21\"]), axis=1)\n",
    "\n",
    "    # lin_prescale= 1/data_raw_lin[\"ids\"].abs().max()\n",
    "\n",
    "    # data_raw_lin[\"ids\"] = data_raw_lin[\"ids\"].apply(lambda x: x*lin_prescale)\n",
    "\n",
    "    trainVal = pd.DataFrame()\n",
    "    test_data = pd.DataFrame()\n",
    "    train_data = pd.DataFrame()\n",
    "    val_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # --- creating the validation dataset for this NN train run, can be different random seed every time\n",
    "    ks_x_trainVal, ks_x_test, ks_y_trainVal, ks_y_test = ks.train_test_split(data_raw[Xlabels], data_raw[Ylabel],test_size=0.10)\n",
    "\n",
    "    ks_x_train, ks_x_val, ks_y_train, ks_y_val = ks.train_test_split(ks_x_trainVal, ks_y_trainVal,test_size=0.15)\n",
    "\n",
    "    train_data[Xlabels] = ks_x_train\n",
    "    train_data[Ylabel] = ks_y_train\n",
    "\n",
    "    val_data[Xlabels] = ks_x_val\n",
    "    val_data[Ylabel] = ks_y_val\n",
    "\n",
    "    test_data[Xlabels] = ks_x_test\n",
    "    test_data[Ylabel] = ks_y_test\n",
    "\n",
    "    X = train_data.loc[:,Xlabels].to_numpy()\n",
    "    y= train_data.loc[:,Ylabel].to_numpy()\n",
    "\n",
    "    X_val = val_data.loc[:,Xlabels].to_numpy()\n",
    "    y_val = val_data.loc[:,Ylabel].to_numpy()\n",
    "\n",
    "    X_test = test_data.loc[:,Xlabels].to_numpy()\n",
    "    y_test = test_data.loc[:,Ylabel].to_numpy()\n",
    "\n",
    "    Xy_train = pd.DataFrame(X.copy())\n",
    "    # Xy_train.columns = [\"Vlat21\", \"Vfglat1\", \"Vtglat1\", \"Vbglat1\"]\n",
    "    Xy_train.rename(columns={0:\"Vlat21\", 1: \"Vfglat1\", 2: \"Vtglat1\"}, inplace=True)\n",
    "    Xy_train[\"ids\"] = y\n",
    "\n",
    "    Xy_val = pd.DataFrame(X_val.copy())\n",
    "    Xy_val.rename(columns={0:\"Vlat21\", 1: \"Vfglat1\", 2: \"Vtglat1\"}, inplace=True)\n",
    "    Xy_val[\"ids\"] = y_val\n",
    "\n",
    "    Xy_test = pd.DataFrame(X_test.copy())\n",
    "    Xy_test.rename(columns={0:\"Vlat21\", 1: \"Vfglat1\", 2: \"Vtglat1\"}, inplace=True)\n",
    "    Xy_test[\"ids\"] = y_test\n",
    "\n",
    "    Xy_train_aug = add_gaussian_noise.augment_train_with_noise(trainVal = Xy_train, num_copies = 3 ,yname=Ylabel[0],sigma=1e-12)    \n",
    "\n",
    "    Xy_train_aug.sort_values(by=[\"ids\"], inplace=True, ignore_index=True)\n",
    "    Xy_train_aug.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Xy_train_aug.columns = [\"Vlat21\", \"Vfglat1\", \"Vtglat1\", \"Vbglat1\", \"ids\"]\n",
    "    Xy_train_aug.rename(columns={0:\"Vlat21\", 1: \"Vfglat1\", 2: \"Vtglat1\", 3: \"ids\"}, inplace=True)\n",
    "\n",
    "    Xy_train_aug_log,_ = transf.transf_Thung(data = Xy_train_aug, Vds=\"Vlat21\", yname = Ylabel[0], improved_del='on', epsilon=1E-3)\n",
    "    Xy_val_log,_ = transf.transf_Thung(data = Xy_val, Vds=\"Vlat21\", yname = Ylabel[0])\n",
    "    Xy_test_log,_ = transf.transf_Thung(data = Xy_test, Vds=\"Vlat21\", yname = Ylabel[0])\n",
    "\n",
    "\n",
    "\n",
    "    # ----building input layer----\n",
    "    x = layers.Input(shape= (3),name =\"InputLayer\")\n",
    "\n",
    "    # ---- hidden layser ----\n",
    "    zOne = layers.Dense(units=pars[\"tf_n_neurons\"], activation = \"tanh\", name = \"HiddenLayer1\")(x)\n",
    "    zTwo = layers.Dense(units=pars[\"tf_n_neurons\"], activation = \"tanh\", name = \"HiddenLayer2\")(zOne)\n",
    "    zThree = layers.Dense(units=pars[\"tf_n_neurons\"], activation = \"tanh\", name = \"HiddenLayer3\")(zTwo)\n",
    "\n",
    "    #output Layer\n",
    "    y = layers.Dense(units = 1, activation = 'linear', name = \"OutputLayer\")(zThree)\n",
    "    \n",
    "    def rmse_loss(y_true, y_pred):\n",
    "        return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "    \n",
    "    #build model\n",
    "    mdl = keras.Model(inputs=x,outputs=y, name = \"NN_model\")\n",
    "    print(mdl.summary())\n",
    "\n",
    "    my_metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "    my_loss = keras.losses.MeanSquaredError()\n",
    "    # my_loss = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "    mdl.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001113),loss=my_loss,metrics = my_metrics)\n",
    "\n",
    "    # compiled_mdl = NN_build_small.build_tanh_model(dim_in = 3, n_neurons = pars[\"tf_n_neurons\"])\n",
    "\n",
    "    callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=pars[\"tf_es_patience\"],min_delta=pars[\"tf_es_delta\"],verbose=1,mode='min',restore_best_weights=True),\n",
    "            tf.keras.callbacks.TensorBoard(\"./NN_tuning_res/RuntimeTest_logs\")]\n",
    "\n",
    "\n",
    "    history = mdl.fit(Xy_train_aug_log[Xlabels],Xy_train_aug_log[\"y_tilde\"],\n",
    "                               batch_size = 100,\n",
    "                               epochs = pars[\"tf_epochs\"],\n",
    "                               callbacks = callbacks,\n",
    "                               validation_data=(Xy_val_log[Xlabels],Xy_val_log[\"y_tilde\"]),\n",
    "                               verbose = 2,\n",
    "                               shuffle = True)\n",
    "\n",
    "\n",
    "    #----- saving the model ----\n",
    "    # mdl,loss,MAE,R2, sMAPE, X, y = NN_run.mx_run_NN_lin(\n",
    "    #                 save_path = pars['save_path'], \n",
    "    #                 epochs = pars['tf_epochs'],\n",
    "    #                 augmentation_type = pars['augmentation_type'],\n",
    "    #                 random_seed = pars[\"train_test_split_random_seed\"],\n",
    "    #                 data = data,\n",
    "    #                 Xlabels = Xlabels,\n",
    "    #                 Ylabel = Ylabel,\n",
    "    #                 es_patience = pars[\"tf_es_patience\"],\n",
    "    #                 es_delta = pars[\"tf_es_delta\"], \n",
    "    #                 _id = _id_lin,\n",
    "    #                 # splitting_algorithm=\"kennard_stone\",\n",
    "    #                 test_size = 0.0,\n",
    "    #                 n_neurons = pars[\"tf_n_neurons\"])\n",
    "    y_pred = mdl.predict(Xy_test_log[Xlabels],batch_size = 50)\n",
    "\n",
    "    loss,MAE= mdl.evaluate(Xy_test_log[Xlabels],Xy_test_log[\"y_tilde\"],batch_size = 100)  # returns loss and metrics\n",
    "\n",
    "    sMAPE = uf.Smape(Xy_test_log[\"y_tilde\"].to_numpy().reshape([-1,1]), y_pred)\n",
    "\n",
    "    # pars[\"lin_prescale\"]                     = lin_prescale\n",
    "\n",
    "    _id_log = mdb_col.insert_one(pars)\n",
    "    _id_log = str(_id_log.inserted_id)\n",
    "    print(_id_log)\n",
    "\n",
    "    mdl.save(pars[\"save_path\"] + \"/\" + str(_id_log))\n",
    "\n",
    "    res.append({\"_id\" : _id_log, \"i\": i,\"MAE\": MAE,  \"loss\": loss, \"model\" : mdl, \"sMAPE\" : sMAPE})\n",
    "    print(f\"i: {i}\\nloss {loss}\\nMAE {MAE}\\n\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mdl.predict(Xy_test_log[Xlabels])\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(Xy_test_log[\"y_tilde\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_res_log = pd.DataFrame(res)\n",
    "\n",
    "pd_res_log.sort_values(by=[\"loss\"], inplace=True, ascending=True)\n",
    "pd_res_log.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(pd_res_log)\n",
    "\n",
    "# get best model\n",
    "best_model_log_id = pd_res_log[\"loss\"].idxmin()\n",
    "best_model_log = pd_res_log.loc[best_model_log_id, \"model\"]\n",
    "\n",
    "_id_log = pd_res_log.loc[best_model_log_id, \"_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "# x_vals = test_data['time']\n",
    "\n",
    "fig, (ax, ax2) = plt.subplots(2, sharex=True)\n",
    "#fig,ax = plt.subplots()\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "#ax2 = ax.twinx()\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "\n",
    "ax2.set_xlabel(\"index\")\n",
    "#ax.set_xlabel(\"time\")\n",
    "\n",
    "# ax.grid()\n",
    "ax2.grid()\n",
    "\n",
    "\n",
    "ys = pd.DataFrame()\n",
    "ys[\"true\"] = y_true\n",
    "\n",
    "ys[\"pred\"] =  mdl.predict(Xy_test_log[Xlabels])\n",
    "# y_true_series = pd.Series(y_true)\n",
    "# y_pred_series = pd.Series(y_pred)\n",
    "\n",
    "ys.sort_values(by=[\"true\"],inplace=True)\n",
    "ys.reset_index(inplace=True)\n",
    "\n",
    "ax.scatter(ys.index,ys[\"true\"].abs(), label=\"data\")\n",
    "ax.scatter(ys.index,ys[\"pred\"].abs(), label=\"model\")\n",
    "\n",
    "error = ys[\"true\"] - ys[\"pred\"]\n",
    "\n",
    "ax2.scatter(ys.index,abs(error))\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R2 current model: %.6f' % skm.r2_score(y_true,y_pred))\n",
    "print('MAE current model: %.4E' %(skm.mean_absolute_error(y_true,y_pred)))\n",
    "print('MSE current model: %.4E' %(skm.mean_squared_error(y_true,y_pred)))\n",
    "print('MAPE current model: %.4E' %(skm.mean_absolute_percentage_error(y_true,y_pred)))\n",
    "print('sMAPE current model: %.6f' %(uf.Smape(y_true.reshape([-1,1]),y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(2, sharex=True)\n",
    "\n",
    "ax.scatter(data_raw.index,data_raw[\"ids\"].abs(), label=\"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_data = data_source.load_from_DC_sim(\"refs/inv_p.plt\",Xlabels, Ylabel, scale_factor=1 )\n",
    "\n",
    "inv_data.sort_values(by=[\"Vlat21\"], inplace=True,ignore_index=True)\n",
    "inv_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, (ax, ax2,ax3,ax4) = plt.subplots(4, sharex=True)\n",
    "#fig,ax = plt.subplots()\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "#ax2 = ax.twinx()\n",
    "\n",
    "\n",
    "# ax2.set_yscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "#ax.set_xlabel(\"time\")\n",
    "\n",
    "# ax.grid()\n",
    "inv_data[\"pred\"] = mdl.predict(inv_data[Xlabels])\n",
    "\n",
    "inv_data[\"pred\"] = -np.multiply(inv_data[\"Vlat21\"], np.exp(inv_data[\"pred\"]) ) * 1E-3\n",
    "\n",
    "\n",
    "# ax.plot(inv_data.index,inv_data[\"pred\"], label=\"Ilin\")\n",
    "ax.plot(inv_data.index,inv_data[\"pred\"], label=\"Ilin\")\n",
    "ax.plot(inv_data.index,-inv_data[\"pred\"], label=\"Ilin-\")\n",
    "\n",
    "ax.plot(inv_data.index,abs(inv_data[\"ids\"]), label=\"ref\")\n",
    "\n",
    "ax2.plot(inv_data.index,inv_data[\"Vlat21\"])\n",
    "ax3.plot(inv_data.index,inv_data[\"Vfglat1\"])\n",
    "\n",
    "# ax.set_ylim([1E-6, 1])\n",
    "# error = abs(val_data[\"ids\"].values.reshape(-1,1) - I)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_data = data_source.load_from_DC_sim(\"refs/inv_p.plt\",Xlabels, Ylabel, scale_factor=1 )\n",
    "\n",
    "inv_data.sort_values(by=[\"Vlat21\"], inplace=True,ignore_index=True)\n",
    "inv_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax, ax2,ax3,ax4) = plt.subplots(4, sharex=True)\n",
    "#fig,ax = plt.subplots()\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "#ax2 = ax.twinx()\n",
    "\n",
    "\n",
    "# ax2.set_yscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "#ax.set_xlabel(\"time\")\n",
    "\n",
    "# ax.grid()\n",
    "inv_data[\"pred\"] = mdl.predict(inv_data[Xlabels])\n",
    "\n",
    "inv_data[\"pred\"] = -np.multiply(inv_data[\"Vlat21\"], np.exp(inv_data[\"pred\"]) ) *1E-3\n",
    "\n",
    "\n",
    "# ax.plot(inv_data.index,inv_data[\"pred\"], label=\"Ilin\")\n",
    "ax.plot(inv_data.index,inv_data[\"pred\"], label=\"Ilin\")\n",
    "ax.plot(inv_data.index,-inv_data[\"pred\"], label=\"Ilin-\")\n",
    "\n",
    "ax.plot(inv_data.index,abs(inv_data[\"ids\"]), label=\"ref\")\n",
    "\n",
    "ax2.plot(inv_data.index,inv_data[\"Vlat21\"])\n",
    "ax3.plot(inv_data.index,inv_data[\"Vfglat1\"])\n",
    "\n",
    "# ax.set_ylim([1E-6, 1])\n",
    "# error = abs(val_data[\"ids\"].values.reshape(-1,1) - I)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_nn = rmdl.rfet_model(\"testname\", template_file= \"templates/verilogA_template_par_A.va\", I_model = {\"lin\" : _id_lin, \"log\" : _id_log}, Q_model = \"668be7cf9144e1dfe0885701\")     # 16 neurons, Q model aus DC Daten\n",
    "\n",
    "mdl_nn.setEnsemblePars(8 , 4) # check hyperparameters in diss\n",
    "mdl_nn.compile(path=\"export/veriloga.va\", significant_digits = 12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
